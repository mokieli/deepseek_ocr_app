# vLLM Direct æ¶æ„ - æ–°å¢/ä¿®æ”¹æ–‡ä»¶æ¸…å•

## ğŸ“ é¡¹ç›®ç»“æ„ï¼ˆæ–°å¢éƒ¨åˆ†ï¼‰

```
deepseek_ocr_app/
â”‚
â”œâ”€â”€ ğŸ“„ .env.vllm-direct                      # é…ç½®æ–‡ä»¶æ¨¡æ¿
â”œâ”€â”€ ğŸ“„ docker-compose.vllm-direct.yml        # Docker Compose é…ç½®
â”œâ”€â”€ ğŸ“„ start-vllm-direct.sh                  # å¿«é€Ÿå¯åŠ¨è„šæœ¬ â­
â”œâ”€â”€ ğŸ“„ docs/vllm-direct/README.md            # è¯¦ç»†ä½¿ç”¨æ–‡æ¡£ â­
â”œâ”€â”€ ğŸ“„ docs/vllm-direct/implementation-summary.md # å®æ–½æ€»ç»“
â”œâ”€â”€ ğŸ“„ docs/vllm-direct/file-manifest.md     # æœ¬æ–‡ä»¶
â”œâ”€â”€ ğŸ“„ docs/vllm-direct/version-compatibility.md # ç‰ˆæœ¬å…¼å®¹æ€§è¯´æ˜
â”‚
â””â”€â”€ backend/
    â”œâ”€â”€ ğŸ“„ Dockerfile.vllm-direct            # å•å®¹å™¨ Dockerfile â­
    â”œâ”€â”€ ğŸ“„ requirements-vllm-direct.txt      # Python ä¾èµ–
    â”‚
    â””â”€â”€ app/
        â”œâ”€â”€ ğŸ“ config.py                     # å·²ä¿®æ”¹ï¼šæ·»åŠ  vLLM Direct é…ç½®
        â”œâ”€â”€ ğŸ“ main.py                       # å·²ä¿®æ”¹ï¼šæ›´æ–°å¯åŠ¨ä¿¡æ¯
        â”‚
        â”œâ”€â”€ api/
        â”‚   â””â”€â”€ ğŸ“ routes.py                 # å·²ä¿®æ”¹ï¼šæ”¯æŒå¤šå¼•æ“
        â”‚
        â”œâ”€â”€ services/
        â”‚   â””â”€â”€ ğŸ“„ vllm_direct_engine.py     # æ ¸å¿ƒï¼švLLM Direct å¼•æ“ â­
        â”‚
        â””â”€â”€ vllm_models/                     # æ–°å»ºï¼šDeepSeek-OCR æ¨¡å‹ä»£ç 
            â”œâ”€â”€ ğŸ“„ __init__.py
            â”œâ”€â”€ ğŸ“„ deepseek_ocr.py           # æ¨¡å‹å®šä¹‰
            â”œâ”€â”€ ğŸ“ config.py                 # å·²ä¿®æ”¹ï¼šé€‚é…åç«¯
            â”‚
            â”œâ”€â”€ process/                     # å›¾åƒå¤„ç†æ¨¡å—
            â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
            â”‚   â”œâ”€â”€ ğŸ“ image_process.py      # å·²ä¿®æ”¹ï¼šå¯¼å…¥è·¯å¾„
            â”‚   â””â”€â”€ ğŸ“„ ngram_norepeat.py
            â”‚
            â””â”€â”€ deepencoder/                 # æ·±åº¦ç¼–ç å™¨
                â”œâ”€â”€ ğŸ“„ __init__.py
                â”œâ”€â”€ ğŸ“„ sam_vary_sdpa.py      # SAM ç¼–ç å™¨
                â”œâ”€â”€ ğŸ“„ clip_sdpa.py          # CLIP ç¼–ç å™¨
                â””â”€â”€ ğŸ“„ build_linear.py       # MLP æŠ•å½±å™¨
```

**å›¾ä¾‹ï¼š**
- ğŸ“„ = æ–°å»ºæ–‡ä»¶
- ğŸ“ = ä¿®æ”¹çš„æ–‡ä»¶
- â­ = å…³é”®æ–‡ä»¶

## ğŸ”‘ å…³é”®æ–‡ä»¶è¯´æ˜

### 1. å¯åŠ¨å’Œé…ç½®

#### `start-vllm-direct.sh` â­
å¿«é€Ÿå¯åŠ¨è„šæœ¬ï¼Œè‡ªåŠ¨æ£€æŸ¥ç¯å¢ƒã€æ„å»ºé•œåƒã€å¯åŠ¨æœåŠ¡ã€‚

**ä½¿ç”¨æ–¹æ³•ï¼š**
```bash
./start-vllm-direct.sh
```

#### `.env.vllm-direct`
é…ç½®æ–‡ä»¶æ¨¡æ¿ï¼ŒåŒ…å«æ‰€æœ‰å¯é…ç½®é¡¹ã€‚

**ä½¿ç”¨æ–¹æ³•ï¼š**
```bash
cp .env.vllm-direct .env
# ç¼–è¾‘ .env æ–‡ä»¶
```

#### `docker-compose.vllm-direct.yml`
Docker Compose é…ç½®ï¼Œå®šä¹‰å•å®¹å™¨æ¶æ„ã€‚

**ä½¿ç”¨æ–¹æ³•ï¼š**
```bash
docker-compose -f docker-compose.vllm-direct.yml up -d
```

### 2. Docker é•œåƒ

#### `backend/Dockerfile.vllm-direct` â­
åŸºäº `vllm/vllm-openai:nightly` çš„ Dockerfileã€‚

**ç‰¹ç‚¹ï¼š**
- ä½¿ç”¨å®˜æ–¹ vLLM é•œåƒ
- å®‰è£… FastAPI å’Œä¾èµ–
- å¤åˆ¶åº”ç”¨ä»£ç å’Œæ¨¡å‹å®šä¹‰

#### `backend/requirements-vllm-direct.txt`
Python ä¾èµ–æ¸…å•ã€‚

**åŒ…å«ï¼š**
- FastAPI åŠç›¸å…³åº“
- å›¾åƒå¤„ç†åº“ï¼ˆPillow, OpenCVï¼‰
- Transformers å’Œ Torch
- å…¶ä»–å·¥å…·åº“

### 3. æ ¸å¿ƒå®ç°

#### `backend/app/services/vllm_direct_engine.py` â­
vLLM Direct å¼•æ“çš„æ ¸å¿ƒå®ç°ã€‚

**ä¸»è¦åŠŸèƒ½ï¼š**
- æ³¨å†Œ DeepSeek-OCR æ¨¡å‹
- åˆå§‹åŒ– AsyncLLMEngine
- å¤„ç†å›¾åƒå’Œæ¨ç†
- ç®¡ç†ç”Ÿå‘½å‘¨æœŸ

**å…³é”®æ–¹æ³•ï¼š**
```python
class VLLMDirectEngine:
    async def load(...)       # åŠ è½½æ¨¡å‹å’Œå¼•æ“
    async def infer(...)      # æ‰§è¡Œæ¨ç†
    async def unload(...)     # å¸è½½å¼•æ“
    def is_loaded(...)        # æ£€æŸ¥çŠ¶æ€
```

### 4. æ¨¡å‹ä»£ç 

#### `backend/app/vllm_models/`
ä» `third_party/DeepSeek-OCR-vllm/` å¤åˆ¶å¹¶é€‚é…çš„æ¨¡å‹ä»£ç ã€‚

**åŒ…å«ï¼š**
- `deepseek_ocr.py`: æ¨¡å‹å®šä¹‰å’Œå¤šæ¨¡æ€å¤„ç†
- `process/`: å›¾åƒé¢„å¤„ç†å’Œ tokenization
- `deepencoder/`: SAM å’Œ CLIP ç¼–ç å™¨
- `config.py`: æ¨¡å‹é…ç½®ï¼ˆå·²é€‚é…ä¸ºä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰

**ä¿®æ”¹ï¼š**
- å¯¼å…¥è·¯å¾„æ”¹ä¸ºç›¸å¯¹å¯¼å…¥
- tokenizer å»¶è¿Ÿåˆå§‹åŒ–
- é…ç½®å‚æ•°ä»ç¯å¢ƒå˜é‡è¯»å–

### 5. åç«¯ä¿®æ”¹

#### `backend/app/config.py` (ä¿®æ”¹)
æ·»åŠ  vLLM Direct é…ç½®é¡¹ã€‚

**æ–°å¢é…ç½®ï¼š**
- `inference_engine`: æ¨ç†å¼•æ“ç±»å‹
- `model_path`: æ¨¡å‹è·¯å¾„
- `tensor_parallel_size`: å¼ é‡å¹¶è¡Œ
- `gpu_memory_utilization`: GPU å†…å­˜åˆ©ç”¨ç‡
- `max_model_len`: æœ€å¤§æ¨¡å‹é•¿åº¦
- `enforce_eager`: Eager æ¨¡å¼

#### `backend/app/api/routes.py` (ä¿®æ”¹)
æ”¯æŒå¤šç§æ¨ç†å¼•æ“ã€‚

**ä¿®æ”¹ï¼š**
- å¯¼å…¥ `VLLMDirectEngine`
- `initialize_service()` æ ¹æ®é…ç½®é€‰æ‹©å¼•æ“
- æ›´æ–°å“åº”ä¸­çš„ `inference_engine` å­—æ®µ

#### `backend/app/main.py` (ä¿®æ”¹)
æ›´æ–°å¯åŠ¨ä¿¡æ¯å’Œç‰ˆæœ¬å·ã€‚

**ä¿®æ”¹ï¼š**
- ç‰ˆæœ¬å·: 3.0.0 â†’ 4.0.0
- å¯åŠ¨ä¿¡æ¯æ˜¾ç¤ºå¼•æ“ç±»å‹å’Œé…ç½®

## ğŸ“š æ–‡æ¡£

### `docs/vllm-direct/README.md` â­
vLLM Direct çš„å¿«é€Ÿå…¥é—¨ä¸æ“ä½œæŒ‡å—ã€‚

**åŒ…å«ï¼š**
- å¿«é€Ÿå¼€å§‹æŒ‡å—
- é…ç½®è¯´æ˜
- å¤šå¡æ¨ç†
- æ€§èƒ½ä¼˜åŒ–
- æ•…éšœæ’æŸ¥
- API ç«¯ç‚¹æ–‡æ¡£

### `docs/vllm-direct/implementation-summary.md`
å®Œæ•´çš„å®æ–½æ€»ç»“ã€‚

**åŒ…å«ï¼š**
- æ¶æ„å˜åŒ–è¯´æ˜
- å®æ–½è¿‡ç¨‹è®°å½•
- å…³é”®ç‰¹æ€§
- ä¼˜åŠ¿æ€»ç»“
- åç»­å»ºè®®

### `docs/vllm-direct/version-compatibility.md`
æ•´ç† vLLM ç‰ˆæœ¬å·®å¼‚ã€å…¼å®¹æ€§è°ƒæ•´ä¸æ’éšœå»ºè®®ã€‚

## ğŸš€ å¿«é€Ÿä½¿ç”¨

### æœ€ç®€å•çš„æ–¹å¼
```bash
# 1. ä½¿ç”¨å¯åŠ¨è„šæœ¬
./start-vllm-direct.sh

# 2. ç­‰å¾…æœåŠ¡å¯åŠ¨ï¼ˆé¦–æ¬¡ä¼šä¸‹è½½æ¨¡å‹ï¼‰
# 3. è®¿é—® http://localhost:8001/docs
```

### æ‰‹åŠ¨æ–¹å¼
```bash
# 1. å¤åˆ¶é…ç½®
cp .env.vllm-direct .env

# 2. æ ¹æ®éœ€è¦ä¿®æ”¹ .env

# 3. å¯åŠ¨æœåŠ¡
docker-compose -f docker-compose.vllm-direct.yml up -d

# 4. æŸ¥çœ‹æ—¥å¿—
docker-compose -f docker-compose.vllm-direct.yml logs -f backend-direct
```

## ğŸ“Š æ–‡ä»¶ç»Ÿè®¡

### æ–°å»ºæ–‡ä»¶
- é…ç½®æ–‡ä»¶: 2 ä¸ª
- Docker æ–‡ä»¶: 2 ä¸ª
- Python ä»£ç : 11 ä¸ª
- æ–‡æ¡£: 3 ä¸ª
- è„šæœ¬: 1 ä¸ª
- **æ€»è®¡: 19 ä¸ªæ–‡ä»¶**

### ä¿®æ”¹æ–‡ä»¶
- Python ä»£ç : 5 ä¸ª
- **æ€»è®¡: 5 ä¸ªæ–‡ä»¶**

### ä»£ç è¡Œæ•°ï¼ˆä¼°ç®—ï¼‰
- Python ä»£ç : ~1200 è¡Œ
- é…ç½®æ–‡ä»¶: ~300 è¡Œ
- æ–‡æ¡£: ~1000 è¡Œ
- **æ€»è®¡: ~2500 è¡Œ**

## âœ… éªŒè¯æ¸…å•

ä½¿ç”¨ä»¥ä¸‹æ¸…å•éªŒè¯å®æ–½æ˜¯å¦æˆåŠŸï¼š

- [ ] æ‰€æœ‰æ–‡ä»¶å·²åˆ›å»º
- [ ] Docker é•œåƒæ„å»ºæˆåŠŸ
- [ ] å®¹å™¨å¯åŠ¨æˆåŠŸ
- [ ] å¥åº·æ£€æŸ¥é€šè¿‡
- [ ] API æ–‡æ¡£å¯è®¿é—® (http://localhost:8001/docs)
- [ ] OCR æ¨ç†æˆåŠŸ
- [ ] å¯ä»¥å¤„ç†å¤§å°ºå¯¸å›¾åƒ
- [ ] æ²¡æœ‰ token é™åˆ¶é”™è¯¯

## ğŸ”— ç›¸å…³æ–‡ä»¶

- ä¸» README: [../../README.md](../../README.md)
- æ¶æ„æ¦‚è§ˆ: [../architecture.md](../architecture.md)
- vLLM Direct æŒ‡å—: [./README.md](./README.md)
- å®æ–½æ€»ç»“: [./implementation-summary.md](./implementation-summary.md)
- ç‰ˆæœ¬å…¼å®¹æ€§: [./version-compatibility.md](./version-compatibility.md)

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **é¦–æ¬¡å¯åŠ¨** ä¼šä¸‹è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ 10-30 åˆ†é’Ÿ
2. **GPU è¦æ±‚** è‡³å°‘éœ€è¦ 8GB VRAMï¼ˆå»ºè®® 16GB+ï¼‰
3. **ç½‘ç»œè¦æ±‚** éœ€è¦è®¿é—® HuggingFace æˆ– ModelScope
4. **å…¼å®¹æ€§** æ”¯æŒåˆ‡æ¢å› vLLM OpenAI æ¨¡å¼

## ğŸ¯ ä¸‹ä¸€æ­¥

1. ä½¿ç”¨ `./start-vllm-direct.sh` å¯åŠ¨æœåŠ¡
2. é˜…è¯» [vLLM Direct æŒ‡å—](./README.md) äº†è§£è¯¦æƒ…
3. æ ¹æ®éœ€è¦è°ƒæ•´é…ç½®æ–‡ä»¶
4. æµ‹è¯• OCR åŠŸèƒ½
5. ç›‘æ§æ€§èƒ½å’Œèµ„æºä½¿ç”¨

---

**å®æ–½å®Œæˆæ—¶é—´ï¼š** 2025-10-30  
**çŠ¶æ€ï¼š** âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ  
**æµ‹è¯•ï¼š** â³ å¾…ç”¨æˆ·éªŒè¯
