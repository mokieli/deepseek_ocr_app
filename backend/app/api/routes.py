"""
API è·¯ç”±
å®šä¹‰æ‰€æœ‰çš„ API ç«¯ç‚¹
"""
import os
from typing import Optional
from fastapi import APIRouter, File, UploadFile, Form, HTTPException, Depends

from ..models.schemas import OCRResponse, HealthResponse, ErrorResponse
from ..services.prompt_builder import PromptBuilder
from ..services.grounding_parser import GroundingParser
from ..services.vllm_direct_engine import VLLMDirectEngine
from ..utils.image_utils import ImageUtils
from ..config import settings


# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter()

# å…¨å±€æ¨ç†æœåŠ¡å®ä¾‹
_inference_service: Optional[VLLMDirectEngine] = None


async def get_inference_service():
    """ä¾èµ–æ³¨å…¥ï¼šè·å–æ¨ç†æœåŠ¡å®ä¾‹"""
    global _inference_service
    if _inference_service is None:
        raise HTTPException(status_code=503, detail="Model not loaded yet")
    return _inference_service


@router.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    engine_info = {
        "message": "DeepSeek-OCR API is running! ğŸš€",
        "docs": "/docs",
        "inference_engine": "vllm_direct",
        "model_path": settings.model_path,
        "vllm_use_v1": settings.vllm_use_v1,
    }

    return engine_info


@router.get("/health", response_model=HealthResponse)
async def health():
    """å¥åº·æ£€æŸ¥"""
    is_loaded = _inference_service is not None and _inference_service.is_loaded()
    return HealthResponse(
        status="healthy" if is_loaded else "starting",
        model_loaded=is_loaded,
        inference_engine="vllm_direct"
    )


@router.post("/api/ocr", response_model=OCRResponse)
async def ocr_inference(
    image: UploadFile = File(..., description="Image file to process"),
    mode: str = Form("plain_ocr", description="OCR mode"),
    prompt: str = Form("", description="Custom prompt for freeform mode"),
    grounding: bool = Form(False, description="Enable grounding boxes"),
    include_caption: bool = Form(False, description="Add image description"),
    find_term: Optional[str] = Form(None, description="Term to find (find_ref mode)"),
    schema: Optional[str] = Form(None, description="JSON schema (kv_json mode)"),
    base_size: int = Form(1024, description="Base processing size"),
    image_size: int = Form(640, description="Image size parameter"),
    crop_mode: bool = Form(True, description="Enable crop mode"),
    test_compress: bool = Form(False, description="Test compression"),
    inference_service = Depends(get_inference_service),
):
    """
    æ‰§è¡Œ OCR æ¨ç†
    """
    tmp_img = None
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„å›¾åƒ
        tmp_img = await ImageUtils.save_upload_file(image)
        
        # è·å–å›¾åƒå°ºå¯¸
        orig_w, orig_h = ImageUtils.get_image_dimensions(tmp_img)
        
        # æ„å»ºæç¤º
        prompt_text = PromptBuilder.build_prompt(
            mode=mode,
            user_prompt=prompt,
            grounding=grounding,
            find_term=find_term,
            schema=schema,
            include_caption=include_caption,
        )
        
        # æ‰§è¡Œæ¨ç†
        raw_text = await inference_service.infer(
            prompt=prompt_text,
            image_path=tmp_img,
            base_size=base_size,
            image_size=image_size,
            crop_mode=crop_mode,
            test_compress=test_compress,
        )
        
        # è§£æè¾¹ç•Œæ¡†
        boxes = []
        if GroundingParser.has_grounding_tags(raw_text):
            boxes = GroundingParser.parse_detections(
                raw_text,
                orig_w or 1,
                orig_h or 1
            )
        
        # æ¸…ç†æ–‡æœ¬
        display_text = raw_text
        if GroundingParser.has_grounding_tags(raw_text):
            display_text = GroundingParser.clean_grounding_text(raw_text)
        
        # å¦‚æœæ¸…ç†åæ²¡æœ‰æ–‡æœ¬ä½†æœ‰è¾¹ç•Œæ¡†ï¼Œæ˜¾ç¤ºæ ‡ç­¾
        if not display_text and boxes:
            display_text = ", ".join([b["label"] for b in boxes])
        
        # æ„å»ºå“åº”
        from ..models.schemas import ImageDimensions, OCRMetadata, BoundingBox
        
        return OCRResponse(
            success=True,
            text=display_text,
            raw_text=raw_text,
            boxes=[BoundingBox(**box) for box in boxes],
            image_dims=ImageDimensions(w=orig_w or 0, h=orig_h or 0) if orig_w and orig_h else None,
            metadata=OCRMetadata(
                mode=mode,
                grounding=grounding or (mode in {"find_ref", "layout_map", "pii_redact"}),
                base_size=base_size,
                image_size=image_size,
                crop_mode=crop_mode,
                inference_engine="vllm_direct",
            )
        )
        
    except Exception as e:
        import traceback
        error_detail = f"{type(e).__name__}: {str(e)}"
        print(f"âŒ Error in OCR inference: {error_detail}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=error_detail)
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if tmp_img and os.path.exists(tmp_img):
            try:
                os.remove(tmp_img)
            except Exception:
                pass


async def initialize_service():
    """åˆå§‹åŒ–æ¨ç†æœåŠ¡ï¼ˆåœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ï¼‰"""
    global _inference_service
    
    print("ğŸ”§ Using vLLM Direct Engine")
    _inference_service = VLLMDirectEngine()
    await _inference_service.load(
        model_path=settings.model_path,
        tensor_parallel_size=settings.tensor_parallel_size,
        gpu_memory_utilization=settings.gpu_memory_utilization,
        max_model_len=settings.max_model_len,
        enforce_eager=settings.enforce_eager,
        use_v1_engine=settings.vllm_use_v1,
    )

async def shutdown_service():
    """å…³é—­æ¨ç†æœåŠ¡ï¼ˆåœ¨åº”ç”¨å…³é—­æ—¶è°ƒç”¨ï¼‰"""
    global _inference_service
    if _inference_service:
        await _inference_service.unload()
        _inference_service = None
