"""
vLLM Direct Engine
ç›´æ¥ä½¿ç”¨ AsyncLLMEngine è¿›è¡Œæ¨ç†ï¼Œé¿å… OpenAI API çš„é™åˆ¶
å‚è€ƒï¼šthird_party/DeepSeek-OCR-vllm/run_dpsk_ocr_image.py
"""
import os
import time
from typing import Optional

import torch
from PIL import Image, ImageOps

from vllm import AsyncLLMEngine, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.model_executor.models.registry import ModelRegistry

# å¯¼å…¥ DeepSeek-OCR æ¨¡å‹å’Œå¤„ç†å™¨
try:
    from vllm.model_executor.models.deepseek_ocr import DeepseekOCRForCausalLM  # type: ignore
    _USING_OFFICIAL_MODEL = True
except ImportError:
    from ..vllm_models.deepseek_ocr import DeepseekOCRForCausalLM  # type: ignore
    _USING_OFFICIAL_MODEL = False

from ..vllm_models.process.image_process import DeepseekOCRProcessor
from ..vllm_models.process.ngram_norepeat import NoRepeatNGramLogitsProcessor
from ..vllm_models import config as vllm_config


class VLLMDirectEngine:
    """ç›´æ¥ä½¿ç”¨ vLLM AsyncLLMEngine çš„æ¨ç†å¼•æ“"""
    
    def __init__(self):
        self.engine: Optional[AsyncLLMEngine] = None
        self.model_path: Optional[str] = None
        self._loaded = False
        self._use_v1_engine = False
        
    def is_loaded(self) -> bool:
        """æ£€æŸ¥å¼•æ“æ˜¯å¦å·²åŠ è½½"""
        return self._loaded and self.engine is not None
    
    async def load(
        self,
        model_path: str,
        tensor_parallel_size: int = 1,
        gpu_memory_utilization: float = 0.75,
        max_model_len: int = 8192,
        enforce_eager: bool = False,
        use_v1_engine: bool = False,
        **kwargs
    ):
        """
        åŠ è½½æ¨¡å‹å’Œåˆå§‹åŒ–å¼•æ“
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹åï¼‰
            tensor_parallel_size: å¼ é‡å¹¶è¡Œå¤§å°
            gpu_memory_utilization: GPU å†…å­˜åˆ©ç”¨ç‡
            max_model_len: æœ€å¤§æ¨¡å‹é•¿åº¦
            enforce_eager: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨ eager æ¨¡å¼
        """
        print(f"ğŸ”§ åˆå§‹åŒ– vLLM Direct Engine...")
        print(f"ğŸ“¦ æ¨¡å‹è·¯å¾„: {model_path}")
        
        self.model_path = model_path
        self._use_v1_engine = use_v1_engine

        os.environ["VLLM_USE_V1"] = "1" if use_v1_engine else "0"
        print(f"ğŸ§  VLLM_USE_V1={os.environ['VLLM_USE_V1']}")
        
        # è®¾ç½® CUDA ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if torch.version.cuda == '11.8':
            os.environ["TRITON_PTXAS_PATH"] = "/usr/local/cuda-11.8/bin/ptxas"

        
        # æ³¨å†Œ DeepSeek-OCR æ¨¡å‹ï¼ˆä»…åœ¨éœ€è¦è‡ªå®šä¹‰å®ç°æ—¶ï¼‰
        if _USING_OFFICIAL_MODEL:
            print("ğŸ“ ä½¿ç”¨ vLLM å†…ç½® DeepSeek-OCR æ¨¡å‹")
        else:
            if "DeepseekOCRForCausalLM" not in ModelRegistry.get_supported_archs():
                print("ğŸ“ æ³¨å†Œè‡ªå®šä¹‰ DeepSeek-OCR æ¨¡å‹...")
                ModelRegistry.register_model("DeepseekOCRForCausalLM", DeepseekOCRForCausalLM)
            else:
                print("â„¹ï¸ è‡ªå®šä¹‰ DeepSeek-OCR æ¨¡å‹å·²æ³¨å†Œï¼Œè·³è¿‡é‡å¤æ³¨å†Œ")
        
        # åˆ›å»ºå¼•æ“å‚æ•°
        engine_args = AsyncEngineArgs(
            model=model_path,
            hf_overrides={"architectures": ["DeepseekOCRForCausalLM"]},
            block_size=256,
            max_model_len=max_model_len,
            enforce_eager=enforce_eager,
            trust_remote_code=True,
            tensor_parallel_size=tensor_parallel_size,
            gpu_memory_utilization=gpu_memory_utilization,
        )
        
        # åˆ›å»ºå¼‚æ­¥å¼•æ“
        print("ğŸš€ åˆ›å»º AsyncLLMEngine...")
        self.engine = AsyncLLMEngine.from_engine_args(engine_args)
        
        self._loaded = True
        print("âœ… vLLM Direct Engine åŠ è½½å®Œæˆ!")
        
    async def unload(self):
        """å¸è½½å¼•æ“"""
        if self.engine:
            print("ğŸ›‘ å¸è½½ vLLM Direct Engine...")
            # vLLM engine æ²¡æœ‰æ˜¾å¼çš„ close æ–¹æ³•ï¼Œåªéœ€è¦è®¾ç½®ä¸º None
            self.engine = None
            self._loaded = False
    
    def _load_image(self, image_path: str) -> Optional[Image.Image]:
        """
        åŠ è½½å›¾åƒå¹¶å¤„ç† EXIF æ—‹è½¬
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            PIL Image å¯¹è±¡
        """
        try:
            image = Image.open(image_path)
            # æ ¹æ® EXIF ä¿¡æ¯è‡ªåŠ¨æ—‹è½¬
            corrected_image = ImageOps.exif_transpose(image)
            return corrected_image
        except Exception as e:
            print(f"âŒ åŠ è½½å›¾åƒå¤±è´¥: {e}")
            try:
                return Image.open(image_path)
            except:
                return None
    
    async def infer(
        self,
        prompt: str,
        image_path: Optional[str] = None,
        base_size: int = 1024,
        image_size: int = 640,
        crop_mode: bool = True,
        temperature: float = 0.0,
        max_tokens: int = 8192,
        test_compress: bool = False,
        **kwargs
    ) -> str:
        """
        æ‰§è¡Œæ¨ç†
        
        Args:
            prompt: æç¤ºæ–‡æœ¬
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            base_size: åŸºç¡€å¤„ç†å°ºå¯¸
            image_size: å›¾åƒå°ºå¯¸å‚æ•°
            crop_mode: æ˜¯å¦å¯ç”¨è£å‰ªæ¨¡å¼
            temperature: é‡‡æ ·æ¸©åº¦
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            test_compress: æ˜¯å¦æµ‹è¯•å‹ç¼©
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if not self.is_loaded():
            raise RuntimeError("Engine æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load()")
        
        # è®¾ç½®é…ç½®å‚æ•°ï¼ˆè¿è¡Œæ—¶è¦†ç›–ï¼‰
        vllm_config.BASE_SIZE = base_size
        vllm_config.IMAGE_SIZE = image_size
        vllm_config.CROP_MODE = crop_mode
        
        # å¤„ç†å›¾åƒï¼ˆå¦‚æœæä¾›ï¼‰
        image_payload = None
        if image_path and '<image>' in prompt:
            image = self._load_image(image_path)
            if image is None:
                raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒ: {image_path}")
            
            # è½¬æ¢ä¸º RGB
            image = image.convert('RGB')
            
            if self._use_v1_engine:
                # vLLM v1 ä¼šåœ¨å†…éƒ¨è°ƒç”¨ DeepseekOCRProcessor å¤„ç†å›¾åƒ
                image_payload = image
            else:
                # ä½¿ç”¨ DeepseekOCRProcessor é¢„å¤„ç†å›¾åƒï¼ˆvLLM legacy è·¯å¾„ï¼‰
                processor = DeepseekOCRProcessor()
                image_payload = processor.tokenize_with_images(
                    images=[image],
                    bos=True,
                    eos=True,
                    cropping=crop_mode
                )
        
        # åˆ›å»ºé‡‡æ ·å‚æ•°
        # NoRepeatNGramLogitsProcessor: é˜²æ­¢é‡å¤ n-gram
        # whitelist_token_ids: <td>, </td> æ ‡ç­¾å…è®¸é‡å¤
        logits_processors = None
        if not self._use_v1_engine:
            logits_processors = [
                NoRepeatNGramLogitsProcessor(
                    ngram_size=30,
                    window_size=90,
                    whitelist_token_ids={128821, 128822}
                )
            ]
        
        sampling_params_kwargs = dict(
            temperature=temperature,
            max_tokens=max_tokens,
            skip_special_tokens=False,
        )
        if logits_processors is not None:
            sampling_params_kwargs["logits_processors"] = logits_processors
        
        sampling_params = SamplingParams(**sampling_params_kwargs)
        
        # æ„å»ºè¯·æ±‚
        request_id = f"request-{int(time.time() * 1000)}"
        
        if image_payload and '<image>' in prompt:
            request = {
                "prompt": prompt,
                "multi_modal_data": {"image": image_payload}
            }
        else:
            request = {
                "prompt": prompt
            }
        
        # æ‰§è¡Œæ¨ç†ï¼ˆæµå¼ï¼‰
        full_text = ""
        async for request_output in self.engine.generate(
            request, sampling_params, request_id
        ):
            if request_output.outputs:
                full_text = request_output.outputs[0].text
        
        return full_text
